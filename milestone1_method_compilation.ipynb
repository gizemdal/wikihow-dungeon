{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "milestone1_method_compilation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V00oq1Gijyfp",
        "colab_type": "text"
      },
      "source": [
        "Add necessary imports here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxqw4tkIjdZm",
        "colab_type": "code",
        "outputId": "43067c04-3af4-4a79-ae91-81346d5c302e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import nltk, string, os, json\n",
        "\n",
        "from nltk import RegexpParser\n",
        "from nltk.tree import Tree\n",
        "\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oZ7xehPlmKN",
        "colab_type": "text"
      },
      "source": [
        "Add necessary dictionaries/lists and parsing methods here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGn2MLXhljpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_article(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        s = [] # steps - list of tuples (method_number, step text)\n",
        "        d = {} # descriptions - Description dictionary step text -> description\n",
        "        m = [] # methods\n",
        "        data_json = json.load(f)\n",
        "        title = data_json['title']\n",
        "        if title != None:\n",
        "        # Get the steps\n",
        "            if len(data_json['method/part']) > 0:\n",
        "                methods = data_json['method/part']\n",
        "                for method in methods:\n",
        "                    all_steps = [(method['name'], step['headline']) for step in method['steps']]\n",
        "                    s += all_steps\n",
        "                    m += method['name']\n",
        "                    all_descs = dict(zip([step['headline'] for step in method['steps']], [step['description'] for step in method['steps']]))\n",
        "                    d.update(all_descs)      \n",
        "            else:\n",
        "              print(filename)\n",
        "               # s = [(None, step[0]) for step in data_json['steps']]\n",
        "               # d = dict(zip([step[0] for step in data_json['steps']], [step[1] for step in data_json['steps']]))\n",
        "        else:\n",
        "            print('Article has no title!')\n",
        "        return s, d, m, title"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG0xYG8mDXoN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4de72cd7-c436-4c2b-c20f-1204fc8c5a0f"
      },
      "source": [
        "nltk.download() # download averaged_perceptron_tagger, wordnet\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tLcCfm4l-A-",
        "colab_type": "text"
      },
      "source": [
        "Add NLTK POS-tagging and tokenizer here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_CsP9T2mDP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Source: https://stackoverflow.com/questions/29473169/approach-for-identifying-whether-a-sentence-includes-an-imperative-within-it?fbclid=IwAR1kGWHvo0M1sSMMXl4Rv4PJ-dhRchEABc83JQpAqotQnFqY3uUBD0jAMAU\n",
        "\n",
        "def is_imperative(tagged_sent):\n",
        "    # if the sentence is not a question...\n",
        "    if tagged_sent[-1][0] != \"?\":\n",
        "         #catches simple imperatives, e.g. \"Open the pod bay doors, HAL!\"\n",
        "        \n",
        "        if tagged_sent[0][1] == \"VB\" or tagged_sent[0][1] == \"MD\":\n",
        "            return True\n",
        "        \n",
        "        # catches imperative sentences starting with words like 'please', 'you',...\n",
        "        # E.g. \"Dave, stop.\", \"Just take a stress pill and think things over.\"\n",
        "        else:\n",
        "          # check if the first word has a verb synonym\n",
        "          synonyms = [syns.name() for syns in wordnet.synsets(tagged_sent[0][0])]\n",
        "          # split the synonyms array into two: more related synonyms should appear earlier\n",
        "          # Follow a heuristic such that if a verb synonym doesn't appear in the first half\n",
        "          # then this word is likely more often used as a noun\n",
        "          synonyms = synonyms[0:len(synonyms)//2]\n",
        "          for synonym in synonyms:\n",
        "            # if there is a verb written the same way as the first token, return true\n",
        "            splitted = synonym.split('.')\n",
        "            if splitted[1] == 'v':\n",
        "              if splitted[0] == tagged_sent[0][0].lower():\n",
        "                return True\n",
        "          #chunk = get_chunks(tagged_sent)\n",
        "          # check if the first chunk of the sentence is a VB-Phrase\n",
        "          #if type(chunk[0]) is Tree and chunk[0].label() == \"VB-Phrase\":\n",
        "              #return True\n",
        "    # For the sake of our game, consider the questions as non-imperative  \n",
        "    return False\n",
        "\n",
        "# chunks the sentence into grammatical phrases based on its POS-tags\n",
        "def get_chunks(tagged_sent):\n",
        "    chunkgram = r\"\"\"VB-Phrase: {<DT><,>*<VB>}\n",
        "                    VB-Phrase: {<RB><VB>}\n",
        "                    VB-Phrase: {<UH><,>*<VB>}\n",
        "                    VB-Phrase: {<UH><,><VBP>}\n",
        "                    VB-Phrase: {<PRP><VB>}\n",
        "                    VB-Phrase: {<NN.?>+<,>*<VB>}\n",
        "                    Q-Tag: {<,><MD><RB>*<PRP><.>*}\"\"\"\n",
        "    chunkparser = RegexpParser(chunkgram)\n",
        "    return chunkparser.parse(tagged_sent)\n",
        "\n",
        "# Tokenizer that I implemented from CIS421\n",
        "def tokenize(text):\n",
        "    tokens = []\n",
        "    words = text.split(' ')\n",
        "    for word in words:\n",
        "        if len(word) == 0:\n",
        "            continue\n",
        "        w = []\n",
        "        for c in word:\n",
        "            if c in string.punctuation:\n",
        "              if c == '\\'':\n",
        "                w.append(c)\n",
        "              else:\n",
        "                  if len(w) != 0:\n",
        "                      tokens.append(''.join(w))\n",
        "                      w = []\n",
        "                  tokens.append(c)\n",
        "            else:\n",
        "                if c not in string.whitespace:\n",
        "                    w.append(c)\n",
        "        if len(w) != 0:\n",
        "            tokens.append(''.join(w))\n",
        "    return tokens\n",
        "\n",
        "def imperative_check(text):\n",
        "  # First tokenize the step headline\n",
        "  tokens = tokenize(text)\n",
        "  # Get the POS-tags\n",
        "  tags = nltk.pos_tag(tokens)\n",
        "  # Check if imperative\n",
        "  imperative = is_imperative(tags)\n",
        "  \n",
        "  return imperative"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkblXFACmbI-",
        "colab_type": "text"
      },
      "source": [
        "Add similarity check functions here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF0bjJ3ameX5",
        "colab_type": "code",
        "outputId": "c5bffd07-575d-4b25-a537-91d51c8811dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!pip3 install pymagnitude\n",
        "!wget http://magnitude.plasticity.ai/glove/heavy/glove.6B.300d.magnitude\n",
        "\n",
        "from pymagnitude import *\n",
        "vectors = Magnitude(\"glove.6B.300d.magnitude\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymagnitude in /usr/local/lib/python3.6/dist-packages (0.1.120)\n",
            "--2020-04-18 02:12:02--  http://magnitude.plasticity.ai/glove/heavy/glove.6B.300d.magnitude\n",
            "Resolving magnitude.plasticity.ai (magnitude.plasticity.ai)... 52.216.29.115\n",
            "Connecting to magnitude.plasticity.ai (magnitude.plasticity.ai)|52.216.29.115|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1384890368 (1.3G) [binary/octet-stream]\n",
            "Saving to: ‘glove.6B.300d.magnitude.1’\n",
            "\n",
            "glove.6B.300d.magni 100%[===================>]   1.29G  16.5MB/s    in 81s     \n",
            "\n",
            "2020-04-18 02:13:24 (16.2 MB/s) - ‘glove.6B.300d.magnitude.1’ saved [1384890368/1384890368]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaGYou_ums8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get vector for the given sentence\n",
        "def construct_sentence_vector(command, vectors):\n",
        "  sentence_vector = np.zeros(shape=(vectors.dim,))\n",
        "  for word in command.split():\n",
        "    word_vector = vectors.query(word)\n",
        "    sentence_vector += word_vector\n",
        "  sentence_vector = sentence_vector / len(command.split())\n",
        "  return sentence_vector\n",
        "\n",
        "#compare items in the list to the article_field. for instance, if you want to find the most similar titles\n",
        "#article_field should be the title, and list should be a list of titles you want to find the most similar out of\n",
        "def find_most_similar(article_field, list, vectors):\n",
        "  vecs = {}\n",
        "  max_sim = 0\n",
        "  most_sim = list[0]\n",
        "  vector_1 = construct_sentence_vector(article_field, vectors)\n",
        "  for obj in list:\n",
        "    vecs[obj] = construct_sentence_vector(obj, vectors)\n",
        "  for obj in vecs:\n",
        "    sim = vectors.similarity(vecs[obj], vector_1)\n",
        "    if sim > max_sim:\n",
        "      most_sim = obj\n",
        "      max_sim = sim\n",
        "  return most_sim\n",
        "\n",
        "#get the top k most similar\n",
        "def top_k_similar(article_field, list, vectors, k):\n",
        "  top = []\n",
        "  for i in range(k):\n",
        "    most_sim = find_most_similar(article_field, list, vectors)\n",
        "    top.append(most_sim)\n",
        "    list.remove(most_sim)\n",
        "  return top\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp_ODobWo8FN",
        "colab_type": "text"
      },
      "source": [
        "Add object identifier & 3rd person sentence formation here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADEpXL8QpB4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# refered from https://stackoverflow.com/questions/41051125/turning-a-sentence-from-first-to-second-person\n",
        "\n",
        "forms = {\"am\" : \"are\", \"are\" : \"am\", 'i' : 'you', 'my' : 'your', 'me' : 'you', 'mine' : 'yours', 'you' : 'I', \n",
        "'your' : 'my', 'yours' : 'mine'}\n",
        "\n",
        "def translate(text):\n",
        "  # print(text)\n",
        "  result = text\n",
        "  for word in text.split():\n",
        "    # print(word)\n",
        "    if word.lower() in forms: \n",
        "      result = result.replace(word, forms[word.lower()])\n",
        "      # print(result)\n",
        "  # print(result)\n",
        "  return result\n",
        "\n",
        "def changePerson(text):\n",
        "  result = \"\"\n",
        "  tags = nltk.pos_tag(tokenize(text))\n",
        "  isImp = is_imperative(tags)\n",
        "\n",
        "  if isImp:\n",
        "    if text[0] == ' ' or text[0] == '\\n':\n",
        "      result = \"You \" + text[1].lower() + text[2:]\n",
        "    else:\n",
        "      result = \"You \" + text[0].lower() + text[1:]\n",
        "  else:\n",
        "    result = translate(text)\n",
        "  return result\n",
        "\n",
        "def identifyObjects(text):\n",
        "  tags = nltk.pos_tag(tokenize(text))\n",
        "  objects = []\n",
        "  for t in tags:\n",
        "    if t[1]==\"NN\":\n",
        "      objects.append(t[0])\n",
        "  return objects"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emb5e4PbkgAq",
        "colab_type": "text"
      },
      "source": [
        "Once imports and functions are done, upload a JSON file, get its name and parse it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrx0bPyhkls_",
        "colab_type": "code",
        "outputId": "4f4ef454-af3f-4607-ca73-b04a2fe47258",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3e01fc09-77ec-409d-8b77-c9f4434084a4\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3e01fc09-77ec-409d-8b77-c9f4434084a4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 3029.json to 3029 (2).json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QHzl_kv-htb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = list(uploaded.keys())[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp1JD5snlA01",
        "colab_type": "code",
        "outputId": "a7d79281-aded-4d82-c0c1-f9878f8ae3d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "def trim_beginning_punctuation(sentence):\n",
        "  to_return = ''\n",
        "  letter_seen = False\n",
        "  for c in sentence:\n",
        "    if c in string.punctuation or c in string.whitespace:\n",
        "      if letter_seen:\n",
        "        to_return += c\n",
        "    else:\n",
        "      if c in string.ascii_letters:\n",
        "        if not letter_seen:\n",
        "          letter_seen = True\n",
        "        to_return += c\n",
        "  return to_return\n",
        "\n",
        "def milestone_1_results(filename):\n",
        "  steps, descriptions, method_names, title = get_article(filename)\n",
        "  # print the title of the article\n",
        "  print('Title: ' + title)\n",
        "  # Once the necessary fields are filled, find imperatives\n",
        "  imperatives = []\n",
        "\n",
        "  for step in steps:\n",
        "    # First split the headline into sentences\n",
        "    headline_sentences = step[1].split('.')\n",
        "    for headline in headline_sentences:\n",
        "      trimmed_headline = trim_beginning_punctuation(headline)\n",
        "      trimmed_headline = trimmed_headline.replace('\\n', '')\n",
        "      if len(trimmed_headline) == 0:\n",
        "          continue\n",
        "      if imperative_check(trimmed_headline):\n",
        "        imperatives.append(trimmed_headline)\n",
        "        break # No two imperatives from the same headline are added.\n",
        "    description = descriptions[step[1]]\n",
        "    # Split the description paragraph into sentences\n",
        "    sentences = description.split('.')\n",
        "    for sentence in sentences:\n",
        "      trimmed_sentence = trim_beginning_punctuation(sentence)\n",
        "      trimmed_sentence = trimmed_sentence.replace('\\n', '')\n",
        "      if len(trimmed_sentence) == 0:\n",
        "        continue\n",
        "      if imperative_check(trimmed_sentence):\n",
        "        imperatives.append(trimmed_sentence)\n",
        "        break # No two imperatives from the same description are added. !!! REMOVE THIS IF you want multiple imperatives\n",
        "\n",
        "    \n",
        "  # Once imperatives are found, print them and turn them into 3rd person\n",
        "  print('\\nList of Imperatives:\\n')\n",
        "  for imp in imperatives:\n",
        "    print(imp)\n",
        "    \n",
        "  third_person = [changePerson(imperative) for imperative in imperatives]\n",
        "\n",
        "  # Print the 3rd person sentences in order\n",
        "  print('\\nImperatives Turned Into 3rd Person:\\n')\n",
        "  for third in third_person:\n",
        "    print(third)\n",
        "  \n",
        "  # # Find 5 similar articles to this article given the title\n",
        "  # k = 5\n",
        "\n",
        "  # # Make sure that the folder new_wikihow_1000 is in the drive - that's where we have all the data\n",
        "  # file_path = '/content/drive/My Drive/Colab Notebooks/new_wikihow_1000'\n",
        "  # all_files = os.listdir(file_path)\n",
        "  # files_and_titles = {} # title -> filename dictionary\n",
        "  # idx = 0\n",
        "  # for data in all_files:\n",
        "  #   if idx >= 100: # for now the number of files to read is limited to 100\n",
        "  #     break\n",
        "  #   with open(file_path + '/' + data, \"r\") as f:\n",
        "  #     data_json = json.load(f)\n",
        "  #     t = data_json['title']\n",
        "  #     # Skip articles with no title\n",
        "  #     if t == None:\n",
        "  #       continue\n",
        "  #     files_and_titles[t] = data\n",
        "  #     idx += 1\n",
        "  \n",
        "  # # Once we have file titles, pass them as a list to top_k_similar()\n",
        "  # k_similar = top_k_similar(title, list(files_and_titles.keys()), vectors, k)\n",
        "  \n",
        "  # # Print the top k similar article titles and the corresponding json file name\n",
        "  # print('\\nTop ' + str(k) + ' Similar Article Titles:\\n')\n",
        "  # for similar in k_similar:\n",
        "  #   print('Title: ' + similar + ', Filename: ' + files_and_titles[similar])\n",
        "\n",
        "milestone_1_results(file_name)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title: How to Get Gigs for Your Band\n",
            "\n",
            "List of Imperatives:\n",
            "\n",
            "Reach out to people in your network\n",
            "Let your friends, family, coworkers, classmates, and anyone else you're acquainted with know that you're in a band and you're looking for gigs\n",
            "Give the promoter a call and tell them your band is interested in doing a show at their venue\n",
            "Call local promoters and ask if you can stop by for a few minutes to drop off a sample of your music\n",
            "Make recordings of your music to play to venues\n",
            "Try to have at least - tracks recorded before you start pitching\n",
            "Follow similar bands on social media and keep track of where they're performing\n",
            "Offer to perform on a weeknight\n",
            "Tell venues you're available for any open slots, including weeknights\n",
            "Take high pay\\/low fan gigs to fund your band\n",
            "Take the money you make from those gigs and use it to pay for the expenses of playing a low pay\\/high fan gig where you'll get more exposure\n",
            "Look for competitions that offer prizes like a slot in a local music festival lineup\n",
            "Ask if you can open for another local band\n",
            "Look online to see what similar bands in your area have upcoming performances\n",
            "Build up your social media presence\n",
            "Use your social media clout as a selling point when pitching to venues\n",
            "Invite the press to your shows\n",
            "\n",
            "Imperatives Turned Into 3rd Person:\n",
            "\n",
            "You reach out to people in your network\n",
            "You let your friends, family, coworkers, classmates, and anyone else you're acquainted with know that you're in a band and you're looking for gigs\n",
            "You give the promoter a call and tell them your band is interested in doing a show at their venue\n",
            "You call local promoters and ask if you can stop by for a few minutes to drop off a sample of your music\n",
            "You make recordings of your music to play to venues\n",
            "You try to have at least - tracks recorded before you start pitching\n",
            "You follow similar bands on social media and keep track of where they're performing\n",
            "You offer to perform on a weeknight\n",
            "You tell venues you're available for any open slots, including weeknights\n",
            "You take high pay\\/low fan gigs to fund your band\n",
            "You take the money you make from those gigs and use it to pay for the expenses of playing a low pay\\/high fan gig where you'll get more exposure\n",
            "You look for competitions that offer prizes like a slot in a local music festival lineup\n",
            "You ask if you can open for another local band\n",
            "You look online to see what similar bands in your area have upcoming performances\n",
            "You build up your social media presence\n",
            "You use your social media clout as a selling point when pitching to venues\n",
            "You invite the press to your shows\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}