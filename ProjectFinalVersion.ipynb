{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjectFinalVersion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJF7YwwCmXzg",
        "colab_type": "code",
        "outputId": "26214c77-6639-4b6f-9221-c8fd71c726f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "!pip3 install pymagnitude\n",
        "# !wget http://magnitude.plasticity.ai/glove/heavy/glove.6B.300d.magnitude\n",
        "\n",
        "from pymagnitude import *"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymagnitude in /usr/local/lib/python3.6/dist-packages (0.1.120)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVMpPp6_mezV",
        "colab_type": "code",
        "outputId": "adcf3cee-f273-40a8-a85b-de296d4f1142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import string\n",
        "import nltk, string, os, json\n",
        "\n",
        "from nltk import RegexpParser\n",
        "from nltk.tree import Tree\n",
        "import random\n",
        "import ipywidgets as widgets\n",
        "\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lijvdogen-7Z",
        "colab_type": "code",
        "outputId": "3f2f971d-c2a4-4e13-f9a0-c903c5775ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViozHWz8v7-P",
        "colab_type": "text"
      },
      "source": [
        "##Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6aQhELymhJC",
        "colab_type": "code",
        "outputId": "22f0d183-e405-487c-e0db-2f8e3ec9581a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)\n",
        "def embed(input):\n",
        "  return model(input)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPlqj9bhmn7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "\n",
        "# def find_similar_command(user_command, known_commands):\n",
        "\n",
        "#   sim1 = find_most_similar_command(user_command, known_commands)\n",
        "#   sim = []\n",
        "#   for k in known_commands:\n",
        "#     sim.append(cosine(model.encode(user_command)[0], model.encode(k)[0]))\n",
        "#   # sim = [float(i)/sum(sim) for i in sim]\n",
        "#   for i in range(len(sim)):\n",
        "#     sim[i] += sim1[i]\n",
        "#   idx = sim.index(max(sim))\n",
        "#   #print(sim)\n",
        "#   #print(idx)\n",
        "#   return known_commands[idx]\n",
        "\n",
        "def find_similar_command(user_command, known_commands):\n",
        "  # TODO - Do something\n",
        "  known_commands_embeddings = embed(known_commands)\n",
        "  known_commands_embeddings = np.array(known_commands_embeddings).tolist()\n",
        "  user_command_embedding = embed([user_command])\n",
        "  user_command_embedding = np.array(user_command_embedding).tolist()[0]\n",
        "\n",
        "  sim = []\n",
        "  for k in known_commands_embeddings:\n",
        "    sim.append(cosine(user_command_embedding, k))\n",
        "  idx = sim.index(max(sim))\n",
        "  return known_commands[idx]\n",
        "  # user_sent = construct_sentence_vector(user_command)\n",
        "  # known_sent = []\n",
        "  # for command in known_commands:\n",
        "  #   known_sent.append(construct_sentence_vector(command))\n",
        "  # #ans = vectors.similarity(user_sent, known_sent) \n",
        "  # known_sent = np.array(known_sent)\n",
        "  # sim = vectors.similarity(user_sent, known_sent).tolist()\n",
        "  # # ind = sim.index(max(sim))\n",
        "  # # print(sim)\n",
        "  # # return known_commands[ind]\n",
        "  # return sim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR8ghViwwAsl",
        "colab_type": "text"
      },
      "source": [
        "##Parse Json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r-ceBOFn5en",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_article(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    s = [] # steps - list of tuples (method_number, step text)\n",
        "    d = {} # descriptions - Description dictionary step text -> description\n",
        "    m = [] # methods\n",
        "    data_json = json.load(f)\n",
        "    title = data_json['title']\n",
        "    title_description = data_json['title_description']\n",
        "    category_list = []\n",
        "    # Get the categories\n",
        "    if data_json['category_hierarchy']:\n",
        "      category_list = data_json['category_hierarchy']\n",
        "    if title != None:\n",
        "    # Get the steps\n",
        "        if len(data_json['method/part']) > 0:\n",
        "            methods = data_json['method/part']\n",
        "            for method in methods:\n",
        "                all_steps = [(method['name'], step['headline']) for step in method['steps']]\n",
        "                s += all_steps\n",
        "                m += method['name']\n",
        "                all_descs = dict(zip([step['headline'] for step in method['steps']], [step['description'] for step in method['steps']]))\n",
        "                d.update(all_descs)      \n",
        "        else:\n",
        "          # Steps could be separate from methods\n",
        "          if data_json['steps']:\n",
        "            for step in data_json['steps']:\n",
        "              s.append(('', step['headline']))\n",
        "              d[step['headline']] = step['description']\n",
        "            # s = [(None, step[0]) for step in data_json['steps']]\n",
        "            # d = dict(zip([step[0] for step in data_json['steps']], [step[1] for step in data_json['steps']]))\n",
        "    else:\n",
        "        print('Article has no title!')\n",
        "    return s, d, m, title, category_list, title_description"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g23wEMSWwD7B",
        "colab_type": "text"
      },
      "source": [
        "##Imperative Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-4Z0v3-oBDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_imperative(tagged_sent):\n",
        "    if tagged_sent[-1][0] != \"?\":\n",
        "         # Catch simple imperatives\n",
        "        if tagged_sent[0][1] == \"VB\" or tagged_sent[0][1] == \"MD\":\n",
        "            return True\n",
        "        elif tagged_sent[0][0].lower() == \"don\\'t\":\n",
        "          # If the first token is don't, then it is imperative as well\n",
        "          return True\n",
        "        else:\n",
        "          possible_verb = ''\n",
        "          # determine which word to check, if first word is an adverb check the second one\n",
        "          if tagged_sent[0][1] == \"RB\":\n",
        "            possible_verb = tagged_sent[1][0]\n",
        "          else:\n",
        "            possible_verb = tagged_sent[0][0]\n",
        "          # check if the first word has a verb synonym\n",
        "          synonyms = [syns.name() for syns in wordnet.synsets(possible_verb)]\n",
        "          # split the synonyms array into two: more related synonyms should appear earlier\n",
        "          # Follow a heuristic such that if a verb synonym doesn't appear in the first half\n",
        "          # then this word is likely more often used as a noun\n",
        "          synonyms = synonyms[0:len(synonyms)//2]\n",
        "          for synonym in synonyms:\n",
        "            # if there is a verb written the same way as the first token, return true\n",
        "            splitted = synonym.split('.')\n",
        "            if splitted[1] == 'v':\n",
        "              if splitted[0] == possible_verb.lower():\n",
        "                return True\n",
        "    # For the sake of our game, consider the questions as non-imperative  \n",
        "    return False\n",
        "\n",
        "# Tokenizer\n",
        "def tokenize(text):\n",
        "    tokens = []\n",
        "    words = text.split(' ')\n",
        "    for word in words:\n",
        "        if len(word) == 0:\n",
        "            continue\n",
        "        w = []\n",
        "        for c in word:\n",
        "            if c in string.punctuation:\n",
        "              if c == '\\'':\n",
        "                w.append(c)\n",
        "              else:\n",
        "                  if len(w) != 0:\n",
        "                      tokens.append(''.join(w))\n",
        "                      w = []\n",
        "                  tokens.append(c)\n",
        "            else:\n",
        "                if c not in string.whitespace:\n",
        "                    w.append(c)\n",
        "        if len(w) != 0:\n",
        "            tokens.append(''.join(w))\n",
        "    return tokens\n",
        "\n",
        "def imperative_check(text):\n",
        "  # First tokenize the step headline\n",
        "  tokens = tokenize(text)\n",
        "  # Get the POS-tags\n",
        "  tags = nltk.pos_tag(tokens)\n",
        "  # Check if imperative\n",
        "  imperative = is_imperative(tags)\n",
        "  \n",
        "  return imperative"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8kfmO4EwG5R",
        "colab_type": "text"
      },
      "source": [
        "## Change to third person"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoteZAvcoJCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# refered from https://stackoverflow.com/questions/41051125/turning-a-sentence-from-first-to-second-person\n",
        "\n",
        "forms = {\"am\" : \"are\", 'i' : 'you', 'my' : 'your', 'me' : 'you', 'mine' : 'yours'} #'you' : 'I', 'your' : 'my', 'yours' : 'mine', \"are\" : \"am\"}\n",
        "\n",
        "def translate(text):\n",
        "  # print(text)\n",
        "  if '\"' in text:\n",
        "    result = \"\"\n",
        "    _text = text.split('\"')\n",
        "    l = len(_text)\n",
        "    for i in range(l):\n",
        "      if i%2==0:\n",
        "        result += _text[i]\n",
        "        for word in _text[i].split():\n",
        "          # print(word)\n",
        "          if word.lower() in forms: \n",
        "            result = result.replace(word, forms[word.lower()])\n",
        "      else:\n",
        "        s = '\"' + _text[i] + '\"'\n",
        "        result+= s\n",
        "    return result\n",
        "\n",
        "  result = text\n",
        "  for word in text.split():\n",
        "    # print(word)\n",
        "    if word.lower() in forms: \n",
        "      result = result.replace(word, forms[word.lower()])\n",
        "      # print(result)\n",
        "  # print(result)\n",
        "  return result\n",
        "\n",
        "def changePerson(text):\n",
        "  result = \"\"\n",
        "  sentences = text.split(\".\")\n",
        "  # print(text)\n",
        "  for _s in sentences:\n",
        "    s = _s.strip()\n",
        "    if s==\"\":\n",
        "      continue\n",
        "    tags = nltk.pos_tag(tokenize(s))\n",
        "    isImp = is_imperative(tags)\n",
        "\n",
        "    r = \"\"\n",
        "    if isImp:\n",
        "      if s[0] == ' ' or s[0] == '\\n':\n",
        "        r += \"You \" + s[1].lower() + s[2:]\n",
        "      else:\n",
        "        r += \"You \" + s[0].lower() + s[1:]\n",
        "    else:\n",
        "      r = s\n",
        "    result += translate(r)\n",
        "    result+='.\\n'\n",
        "  return result\n",
        "\n",
        "def identifyObjects(text):\n",
        "  tags = nltk.pos_tag(tokenize(text))\n",
        "  objects = []\n",
        "  for t in tags:\n",
        "    if t[1]==\"NN\":\n",
        "      objects.append(t[0])\n",
        "  return objects\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiA3qK4bwPRj",
        "colab_type": "text"
      },
      "source": [
        "## Description Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7c9_fe9v5E7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_description_title(title):\n",
        "  tit=\"\"\n",
        "  title = title.lower()\n",
        "  if len(title) == 0:\n",
        "    return\n",
        "  if title.split()[0].lower()==\"how\" and title.split()[1].lower()==\"to\":\n",
        "    tit += \"You want \"\n",
        "    tit += \" \".join(title.split()[1:])\n",
        "  print(tit)\n",
        "\n",
        "def get_description_title_description(title_description):\n",
        "  if title_description is None or title_description==\"\":\n",
        "    return\n",
        "  print(changePerson(title_description))\n",
        "\n",
        "def get_step_description_game(step, description):\n",
        "  des = description[step]\n",
        "  # print(des)\n",
        "  if(des==\"\"):\n",
        "    return\n",
        "  d = changePerson(des)\n",
        "  return d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y0uDyXmxEwF",
        "colab_type": "text"
      },
      "source": [
        "##Wrong Choices/Similar Steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pebi4f3uwyBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_steps_similar_articles(title, categories=[]):\n",
        "  vectors = Magnitude(\"drive/Shared drives/CIS 700 Project/uniemb.magnitude\")\n",
        "  # v=construct_sentence_vector(title, vectors)\n",
        "  sen_vec = embed([title])\n",
        "  v = np.array(sen_vec).tolist()[0]\n",
        "  sim = vectors.most_similar(v, topn = 10)\n",
        "  steps = []\n",
        "  cnt=0\n",
        "  \n",
        "  for s in sim:\n",
        "    f = open('drive/Shared drives/CIS 700 Project/WikihowData/js_files/' + s[0] + '.json')\n",
        "    data_json = json.load(f) \n",
        "    \n",
        "    tit = data_json['title']\n",
        "    cats = data_json['category_hierarchy']\n",
        "    if categories==None or len(categories)==0 or len(list(set(cats) & set(categories)))>0:\n",
        "      if tit.lower()!=title.lower() and cnt<3:\n",
        "        cnt+=1\n",
        "        if len(data_json['method/part']) > 0:\n",
        "          methods = data_json['method/part']\n",
        "          for method in methods:\n",
        "              all_steps = [step['headline'] for step in method['steps']]\n",
        "              steps += all_steps \n",
        "        else:\n",
        "          if data_json['steps']:\n",
        "            for step in data_json['steps']:\n",
        "              steps.append(step['headline'])       \n",
        "  return steps\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_EqOkHPxWAU",
        "colab_type": "text"
      },
      "source": [
        "## Subcategories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7Fd01u2xJYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stored the sub categories data in categories.json\n",
        "#just load the data for categories \n",
        "with open('drive/Shared drives/CIS 700 Project/categories.json', 'r') as f:\n",
        "  sub_categories = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4dmoU9dxdsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in sub_categories.keys():\n",
        "  print(k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B22hWf3GyDm9",
        "colab_type": "code",
        "outputId": "4f0e7b7f-e51e-47a6-a494-71bb6c49ab9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "total_files = 0\n",
        "for k in sub_categories.keys():\n",
        "  total_files += len(sub_categories[k])\n",
        "print(total_files)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMSzvPKfzGtM",
        "colab_type": "text"
      },
      "source": [
        "## Gameplay functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWNVIqE3yeRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trim_beginning_punctuation(sentence):\n",
        "  to_return = ''\n",
        "  letter_seen = False\n",
        "  for c in sentence:\n",
        "    if c in string.punctuation or c in string.whitespace:\n",
        "      if letter_seen:\n",
        "        to_return += c\n",
        "    else:\n",
        "      if c in string.ascii_letters or c in string.digits:\n",
        "        if not letter_seen:\n",
        "          letter_seen = True\n",
        "        to_return += c\n",
        "  return to_return\n",
        "\n",
        "def milestone_1_results(filename, include_desc):\n",
        "  # include_desc -> Do you want to include imperative description sentences as game steps?\n",
        "  steps, descriptions, method_names, title, category_list, title_dis = get_article(filename)\n",
        "  # Once the necessary fields are filled, find imperatives\n",
        "  imperatives = []\n",
        "\n",
        "  for step in steps:\n",
        "    # First split the headline into sentences\n",
        "    headline_sentences = step[1].split('.')\n",
        "    if len(headline_sentences) == 1:\n",
        "      headline_sentences = headline_sentences[0].split(':')\n",
        "    for headline in headline_sentences:\n",
        "      trimmed_headline = trim_beginning_punctuation(headline)\n",
        "      trimmed_headline = trimmed_headline.replace('\\n', '')\n",
        "      if len(trimmed_headline) == 0:\n",
        "          continue\n",
        "      if imperative_check(trimmed_headline):\n",
        "        if trimmed_headline not in imperatives:\n",
        "          imperatives.append(trimmed_headline)\n",
        "          # fix the way it is represented in descriptions\n",
        "          old_desc = descriptions[step[1]]\n",
        "          del descriptions[step[1]]\n",
        "          descriptions[trimmed_headline] = old_desc\n",
        "        break # No two imperatives from the same headline are added.\n",
        "    if include_desc:\n",
        "      description = descriptions[step[1]]\n",
        "      # Split the description paragraph into sentences\n",
        "      sentences = description.split('.')\n",
        "      for sentence in sentences:\n",
        "        trimmed_sentence = trim_beginning_punctuation(sentence)\n",
        "        trimmed_sentence = trimmed_sentence.replace('\\n', '')\n",
        "        if len(trimmed_sentence) == 0:\n",
        "          continue\n",
        "        if imperative_check(trimmed_sentence):\n",
        "          if trimmed_sentence not in imperatives:\n",
        "            imperatives.append(trimmed_sentence)\n",
        "          break # No two imperatives from the same description are added. !!! REMOVE THIS IF you want multiple imperatives\n",
        "    \n",
        "  #third_person = [changePerson(imperative) for imperative in imperatives]\n",
        "  return imperatives, title, descriptions, category_list, title_dis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0sbB8n4zW8Z",
        "colab_type": "text"
      },
      "source": [
        "Choice picking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61YW2NDTzQrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pick_choices(current_step, commands):\n",
        "  if len(commands) - 1 == current_step:\n",
        "    return None, None\n",
        "  elif len(commands) - 2 == current_step:\n",
        "    # Can only pick one wrong choice\n",
        "    return len(commands) - 1, None\n",
        "  else:\n",
        "    # pick 2 that are not in steps_seen\n",
        "    options_first = [num for num in range(current_step + 1, len(commands))]\n",
        "    first = random.choice(options_first)\n",
        "    options_second = [num for num in range(current_step + 1, len(commands)) if num != first]\n",
        "    second = random.choice(options_second)\n",
        "    return first,second\n",
        "\n",
        "def pick_choices_alt(title, n, similars, curr_step, further_steps):\n",
        "  # first filter the ones that are too similar to actual correct step\n",
        "  user_sent = embed([curr_step])\n",
        "  user_sent = np.array(user_sent).tolist()[0]\n",
        "  # user_sent = construct_sentence_vector(curr_step)\n",
        "  similar_steps = embed(similars)\n",
        "  similar_steps = np.array(similar_steps).tolist()\n",
        "\n",
        "  s = []\n",
        "  for idx, k in enumerate(similar_steps):\n",
        "    a = cosine(user_sent, k)\n",
        "    if a<0.9 and a>0.1:\n",
        "      s.append(similars[idx])\n",
        "  # s = [sim for sim in similars if vectors.similarity(construct_sentence_vector(sim), user_sent) < 0.9 and vectors.similarity(construct_sentence_vector(sim), user_sent) > 0.1]\n",
        "  s = s + further_steps\n",
        "  first = random.choice(s)\n",
        "  s_2 = [step for step in s if step != first]\n",
        "  second = random.choice(s_2)\n",
        "  return first,second"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_F-8Mua1DtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrong_warning = ['Whoa! You should not do that yet.', 'Umm.. maybe you should not do that now.', 'That could work in an ideal world, but that is not what we have here. Try again please!', 'No no no...']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8-bG0X62m0k",
        "colab_type": "text"
      },
      "source": [
        "##Game Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbm51bPG2ktz",
        "colab_type": "code",
        "outputId": "f6482076-e584-4be3-bcc3-c8cdeb04b66b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 676
        }
      },
      "source": [
        "current_step = 0\n",
        "steps_seen = []\n",
        "end_game = False\n",
        "file_name = ''\n",
        "# First get the game\n",
        "print('Welcome to WikiHow Dungeon!')\n",
        "print('You can either pick a category to play a game from or you can upload your own game.')\n",
        "print('a) Pick a category')\n",
        "print('b) Upload JSON')\n",
        "picked = ''\n",
        "is_picked = False\n",
        "while not is_picked:\n",
        "  picked = input('>')\n",
        "  if picked.lower() == 'a' or picked.lower() == 'b':\n",
        "    is_picked = True\n",
        "  else:\n",
        "    print('Try again.')\n",
        "# If from category\n",
        "if picked.lower() == 'a':\n",
        "  print('Enter the corresponding letter for a category below:')\n",
        "  print('a) School: university, other school stuff')\n",
        "  print('b) Youth: dating, culture, interactions, social events')\n",
        "  print('c) Work: going to work, socializing at work')\n",
        "  print('d) Animals: dogs, cats, birds')\n",
        "  print('e) Fun: activities, parties, toys, music, drinks, games, halloween')\n",
        "  print('f) Home and Family: parents, weddings, neighbors')\n",
        "  print('g) Pop Culture: fandom, celebrities')\n",
        "  print('h) Relationships: single life, dating, relationship issues')\n",
        "  print('i) Friendship: friends, traveling with companions, nicknames')\n",
        "  print('k) Appearance: fashion, looking good')\n",
        "  is_picked = False\n",
        "  while not is_picked:\n",
        "    picked = input('>')\n",
        "    if len(picked) == 1 and picked.lower() in 'abcdefghik':\n",
        "      is_picked = True\n",
        "    else:\n",
        "      print('Try again.')\n",
        "  categories = []\n",
        "  if picked.lower() == 'a':\n",
        "    categories = ['University', 'School Stuff']\n",
        "  elif picked.lower() == 'b':\n",
        "    categories = ['Youth Dating', 'Youth Culture', 'Social Interactions for Youth', 'Social Events for Youth']\n",
        "  elif picked.lower() == 'c':\n",
        "    categories = ['Job Attendance', 'Socializing at Work', 'School Stuff']\n",
        "  elif picked.lower() == 'd':\n",
        "    categories = ['Dogs', 'Cats', 'Birds']\n",
        "  elif picked.lower() == 'e':\n",
        "    categories = ['Drinks', 'Games', 'Parties', 'Fun Activities', 'Toys', 'Halloween', 'Music']\n",
        "  elif picked.lower() == 'f':\n",
        "    categories = ['Youth and Family', 'Parents', 'School Stuff', 'Weddings', 'Neighbors']\n",
        "  elif picked.lower() == 'g':\n",
        "    categories = ['Celebrities', 'Fandom']\n",
        "  elif picked.lower() == 'h':\n",
        "    categories = ['Relationship Issues', 'Single Life', 'Dating']\n",
        "  elif picked.lower() == 'i':\n",
        "    categories = ['Friends', 'Social Interactions', 'Traveling with Companions', 'Nicknames']\n",
        "  elif picked.lower() == 'k':\n",
        "    categories = ['Fashion', 'Looking Good']\n",
        "  c = random.choice(categories)\n",
        "  articles = sub_categories[c]\n",
        "  found_imperatives = False\n",
        "  while not found_imperatives:\n",
        "    file_name = random.choice(articles)\n",
        "    file_name = \"/content/drive/Shared drives/CIS 700 Project/category_files/\" + file_name\n",
        "    # Get the commands, descriptions and title\n",
        "    known_commands, title, descriptions, category_list, title_description = milestone_1_results(file_name, False)\n",
        "    if len(known_commands) > 0:\n",
        "      found_imperatives = True\n",
        "\n",
        "elif picked.lower() == 'b':\n",
        "  # Upload a file\n",
        "  is_valid_file = False\n",
        "  while not is_valid_file:\n",
        "    is_valid = True\n",
        "    uploaded = files.upload()\n",
        "    try:\n",
        "      file_name = list(uploaded.keys())[0]\n",
        "      if file_name.split('.')[1] != 'json':\n",
        "        is_valid = False\n",
        "      else:\n",
        "          with open(file_name, \"r\") as f:\n",
        "            data_json = json.load(f)\n",
        "            fields = list(data_json.keys())\n",
        "            if 'title' not in fields or 'method/part' not in fields:\n",
        "              is_valid = False\n",
        "    except:\n",
        "      is_valid = False\n",
        "    if is_valid:\n",
        "      is_valid_file = True\n",
        "      break\n",
        "    print('Invalid file, please try again.')\n",
        "  # Get the commands, descriptions and title\n",
        "  known_commands, title, descriptions, category_list, title_description = milestone_1_results(file_name, False)\n",
        "\n",
        "print('Game: ' + title + '\\n')\n",
        "get_description_title(title)\n",
        "get_description_title_description(title_description)\n",
        "similars = find_steps_similar_articles(title)\n",
        "#similars = []\n",
        "while not end_game:\n",
        "  if current_step == len(known_commands):\n",
        "    end_game = True\n",
        "    print('You reached the end of the game')\n",
        "  else:\n",
        "    if current_step == 0:\n",
        "      print('What do you do first?')\n",
        "    else:\n",
        "      print('What do you do next?')\n",
        "    print('You:')\n",
        "    #first, second = pick_choices(current_step, known_commands)\n",
        "    choices = []\n",
        "    correct_choice = known_commands[current_step] # this is the correct choice\n",
        "    further_steps = [known_commands[i] for i in range(current_step + 1, len(known_commands))]\n",
        "    f, s = pick_choices_alt(title, 2, similars, correct_choice, further_steps)\n",
        "    #f, s = pick_choices(curr_step, known_commands)\n",
        "    if f:\n",
        "      choices.append(f)\n",
        "    if s:\n",
        "      choices.append(s)\n",
        "    choices.append(correct_choice)\n",
        "    random.shuffle(choices) # shuffle the order of choices\n",
        "    for choice in choices:\n",
        "      print('\\t' + choice.lower())\n",
        "    correct_found = False\n",
        "    while not correct_found:\n",
        "      user_pick = input('>')\n",
        "      if user_pick.lower() == 'quit':\n",
        "        correct_found = True\n",
        "        end_game = True\n",
        "        print('You ended the game.')\n",
        "        continue\n",
        "      # Check if what the user picked is correct\n",
        "      most_similar = find_similar_command(user_pick, [known_commands[i] for i in range(current_step, len(known_commands))])\n",
        "      if most_similar.lower() == correct_choice.lower():\n",
        "        # Correct!\n",
        "        current_step += 1\n",
        "        correct_found = True\n",
        "        # Get description\n",
        "        print(get_step_description_game(correct_choice, descriptions))\n",
        "      else:\n",
        "        print(random.choice(wrong_warning))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome to WikiHow Dungeon!\n",
            "You can either pick a category to play a game from or you can upload your own game.\n",
            "a) Pick a category\n",
            "b) Upload JSON\n",
            ">b\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-65ee857c-3303-486c-b53f-28b5b545a13f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-65ee857c-3303-486c-b53f-28b5b545a13f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving how_to_safely_hug_your_dog.json to how_to_safely_hug_your_dog.json\n",
            "Game: How To Safely Hug Your Dog\n",
            "\n",
            "You want to safely hug your dog\n",
            "What do you do first?\n",
            "You:\n",
            "\tgive your dog affection\n",
            "\tget the dogs shots, and make sure he\\/she has all of them before you go out and about\n",
            "\tgive them a warm, petting collar if you want to give them a touch\n",
            ">give them a warm, petting collar if you want to give them a touch\n",
            "It’s a good idea to give your dog treats more frequently than every 2 hours or even every hour.\n",
            "If they don’t want to get too hot or hot, they should simply be petted and held a little, as long as they wait for a pat.\n",
            "If it’s too hot, you might need to hold your dog for a closer look or give them a longer, petting collar just for them to get used t.\n",
            "\n",
            "What do you do next?\n",
            "You:\n",
            "\thave someone photograph you while hugging your dog\n",
            "\tgive your dog lots of attention and love\n",
            "\tleave your dog alone\n",
            ">give your dog lots of attention and love\n",
            "Umm.. maybe you should not do that now.\n",
            ">leave your dog alone\n",
            "Even if they don’t bite, your dog's instincts often get the better of them.\n",
            "Most dogs won't go to extreme lengths to protect themselves if it means they have peace for just a second.\n",
            "You keep it in mind as this will help your dog get used to you and calm them down more than any force-feeding or grooming.\n",
            "\n",
            "What do you do next?\n",
            "You:\n",
            "\tgive your dog plenty of space and time to sit\n",
            "\tgive your pet a little sit down & a roll over rest so she\\/he can not be puffed for the trip home\n",
            "\ttake her to the vet\n",
            ">quit\n",
            "You ended the game.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}