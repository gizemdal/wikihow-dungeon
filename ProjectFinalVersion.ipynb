{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjectFinalVersion.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJF7YwwCmXzg",
        "colab_type": "code",
        "outputId": "28ca0e7f-7bc1-4083-879b-1778d6a32978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip3 install pymagnitude\n",
        "# !wget http://magnitude.plasticity.ai/glove/heavy/glove.6B.300d.magnitude\n",
        "\n",
        "from pymagnitude import *"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymagnitude in /usr/local/lib/python3.6/dist-packages (0.1.120)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVMpPp6_mezV",
        "colab_type": "code",
        "outputId": "40deb941-ee73-4b92-e91a-e3c28f7daf70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import string\n",
        "import nltk, string, os, json\n",
        "\n",
        "from nltk import RegexpParser\n",
        "from nltk.tree import Tree\n",
        "import random\n",
        "import ipywidgets as widgets\n",
        "\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lijvdogen-7Z",
        "colab_type": "code",
        "outputId": "76654dff-6f46-4040-ca42-bb2b5b35de7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViozHWz8v7-P",
        "colab_type": "text"
      },
      "source": [
        "##Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6aQhELymhJC",
        "colab_type": "code",
        "outputId": "3110b8db-5b94-428a-a160-1871847775cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)\n",
        "def embed(input):\n",
        "  return model(input)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPlqj9bhmn7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "\n",
        "# def find_similar_command(user_command, known_commands):\n",
        "\n",
        "#   sim1 = find_most_similar_command(user_command, known_commands)\n",
        "#   sim = []\n",
        "#   for k in known_commands:\n",
        "#     sim.append(cosine(model.encode(user_command)[0], model.encode(k)[0]))\n",
        "#   # sim = [float(i)/sum(sim) for i in sim]\n",
        "#   for i in range(len(sim)):\n",
        "#     sim[i] += sim1[i]\n",
        "#   idx = sim.index(max(sim))\n",
        "#   #print(sim)\n",
        "#   #print(idx)\n",
        "#   return known_commands[idx]\n",
        "\n",
        "def find_similar_command(user_command, known_commands):\n",
        "  # TODO - Do something\n",
        "  known_commands_embeddings = embed(known_commands)\n",
        "  known_commands_embeddings = np.array(known_commands_embeddings).tolist()\n",
        "  user_command_embedding = embed([user_command])\n",
        "  user_command_embedding = np.array(user_command_embedding).tolist()[0]\n",
        "\n",
        "  sim = []\n",
        "  for k in known_commands_embeddings:\n",
        "    sim.append(cosine(user_command_embedding, k))\n",
        "  idx = sim.index(max(sim))\n",
        "  return known_commands[idx]\n",
        "  # user_sent = construct_sentence_vector(user_command)\n",
        "  # known_sent = []\n",
        "  # for command in known_commands:\n",
        "  #   known_sent.append(construct_sentence_vector(command))\n",
        "  # #ans = vectors.similarity(user_sent, known_sent) \n",
        "  # known_sent = np.array(known_sent)\n",
        "  # sim = vectors.similarity(user_sent, known_sent).tolist()\n",
        "  # # ind = sim.index(max(sim))\n",
        "  # # print(sim)\n",
        "  # # return known_commands[ind]\n",
        "  # return sim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR8ghViwwAsl",
        "colab_type": "text"
      },
      "source": [
        "##Parse Json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r-ceBOFn5en",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_article(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    s = [] # steps - list of tuples (method_number, step text)\n",
        "    d = {} # descriptions - Description dictionary step text -> description\n",
        "    m = [] # methods\n",
        "    data_json = json.load(f)\n",
        "    title = data_json['title']\n",
        "    title_description = data_json['title_description']\n",
        "    category_list = []\n",
        "    # Get the categories\n",
        "    if data_json['category_hierarchy']:\n",
        "      category_list = data_json['category_hierarchy']\n",
        "    if title != None:\n",
        "    # Get the steps\n",
        "        if len(data_json['method/part']) > 0:\n",
        "            methods = data_json['method/part']\n",
        "            for method in methods:\n",
        "                all_steps = [(method['name'], step['headline']) for step in method['steps']]\n",
        "                s += all_steps\n",
        "                m += method['name']\n",
        "                all_descs = dict(zip([step['headline'] for step in method['steps']], [step['description'] for step in method['steps']]))\n",
        "                d.update(all_descs)      \n",
        "        else:\n",
        "          # Steps could be separate from methods\n",
        "          if data_json['steps']:\n",
        "            for step in data_json['steps']:\n",
        "              s.append(('', step['headline']))\n",
        "              d[step['headline']] = step['description']\n",
        "            # s = [(None, step[0]) for step in data_json['steps']]\n",
        "            # d = dict(zip([step[0] for step in data_json['steps']], [step[1] for step in data_json['steps']]))\n",
        "    else:\n",
        "        print('Article has no title!')\n",
        "    return s, d, m, title, category_list, title_description"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g23wEMSWwD7B",
        "colab_type": "text"
      },
      "source": [
        "##Imperative Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-4Z0v3-oBDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_imperative(tagged_sent):\n",
        "    if tagged_sent[-1][0] != \"?\":\n",
        "         # Catch simple imperatives\n",
        "        if tagged_sent[0][1] == \"VB\" or tagged_sent[0][1] == \"MD\":\n",
        "            return True\n",
        "        elif tagged_sent[0][0].lower() == \"don\\'t\":\n",
        "          # If the first token is don't, then it is imperative as well\n",
        "          return True\n",
        "        else:\n",
        "          possible_verb = ''\n",
        "          # determine which word to check, if first word is an adverb check the second one\n",
        "          if tagged_sent[0][1] == \"RB\":\n",
        "            possible_verb = tagged_sent[1][0]\n",
        "          else:\n",
        "            possible_verb = tagged_sent[0][0]\n",
        "          # check if the first word has a verb synonym\n",
        "          synonyms = [syns.name() for syns in wordnet.synsets(possible_verb)]\n",
        "          # split the synonyms array into two: more related synonyms should appear earlier\n",
        "          # Follow a heuristic such that if a verb synonym doesn't appear in the first half\n",
        "          # then this word is likely more often used as a noun\n",
        "          synonyms = synonyms[0:len(synonyms)//2]\n",
        "          for synonym in synonyms:\n",
        "            # if there is a verb written the same way as the first token, return true\n",
        "            splitted = synonym.split('.')\n",
        "            if splitted[1] == 'v':\n",
        "              if splitted[0] == possible_verb.lower():\n",
        "                return True\n",
        "    # For the sake of our game, consider the questions as non-imperative  \n",
        "    return False\n",
        "\n",
        "# Tokenizer\n",
        "def tokenize(text):\n",
        "    tokens = []\n",
        "    words = text.split(' ')\n",
        "    for word in words:\n",
        "        if len(word) == 0:\n",
        "            continue\n",
        "        w = []\n",
        "        for c in word:\n",
        "            if c in string.punctuation:\n",
        "              if c == '\\'':\n",
        "                w.append(c)\n",
        "              else:\n",
        "                  if len(w) != 0:\n",
        "                      tokens.append(''.join(w))\n",
        "                      w = []\n",
        "                  tokens.append(c)\n",
        "            else:\n",
        "                if c not in string.whitespace:\n",
        "                    w.append(c)\n",
        "        if len(w) != 0:\n",
        "            tokens.append(''.join(w))\n",
        "    return tokens\n",
        "\n",
        "def imperative_check(text):\n",
        "  # First tokenize the step headline\n",
        "  tokens = tokenize(text)\n",
        "  # Get the POS-tags\n",
        "  tags = nltk.pos_tag(tokens)\n",
        "  # Check if imperative\n",
        "  imperative = is_imperative(tags)\n",
        "  \n",
        "  return imperative"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8kfmO4EwG5R",
        "colab_type": "text"
      },
      "source": [
        "## Change to third person"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoteZAvcoJCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# refered from https://stackoverflow.com/questions/41051125/turning-a-sentence-from-first-to-second-person\n",
        "\n",
        "forms = {\"am\" : \"are\", 'i' : 'you', 'my' : 'your', 'me' : 'you', 'mine' : 'yours'} #'you' : 'I', 'your' : 'my', 'yours' : 'mine', \"are\" : \"am\"}\n",
        "\n",
        "def translate(text):\n",
        "  # print(text)\n",
        "  if '\"' in text:\n",
        "    result = \"\"\n",
        "    _text = text.split('\"')\n",
        "    l = len(_text)\n",
        "    for i in range(l):\n",
        "      if i%2==0:\n",
        "        result += _text[i]\n",
        "        for word in _text[i].split():\n",
        "          # print(word)\n",
        "          if word.lower() in forms: \n",
        "            result = result.replace(word, forms[word.lower()])\n",
        "      else:\n",
        "        s = '\"' + _text[i] + '\"'\n",
        "        result+= s\n",
        "    return result\n",
        "\n",
        "  result = text\n",
        "  for word in text.split():\n",
        "    # print(word)\n",
        "    if word.lower() in forms: \n",
        "      result = result.replace(word, forms[word.lower()])\n",
        "      # print(result)\n",
        "  # print(result)\n",
        "  return result\n",
        "\n",
        "def changePerson(text):\n",
        "  result = \"\"\n",
        "  sentences = text.split(\".\")\n",
        "  # print(text)\n",
        "  for _s in sentences:\n",
        "    s = _s.strip()\n",
        "    if s==\"\":\n",
        "      continue\n",
        "    tags = nltk.pos_tag(tokenize(s))\n",
        "    isImp = is_imperative(tags)\n",
        "\n",
        "    r = \"\"\n",
        "    if isImp:\n",
        "      if s[0] == ' ' or s[0] == '\\n':\n",
        "        r += \"You \" + s[1].lower() + s[2:]\n",
        "      else:\n",
        "        r += \"You \" + s[0].lower() + s[1:]\n",
        "    else:\n",
        "      r = s\n",
        "    result += translate(r)\n",
        "    result+='.\\n'\n",
        "  return result\n",
        "\n",
        "def identifyObjects(text):\n",
        "  tags = nltk.pos_tag(tokenize(text))\n",
        "  objects = []\n",
        "  for t in tags:\n",
        "    if t[1]==\"NN\":\n",
        "      objects.append(t[0])\n",
        "  return objects\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiA3qK4bwPRj",
        "colab_type": "text"
      },
      "source": [
        "## Description Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7c9_fe9v5E7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_description_title(title):\n",
        "  tit=\"\"\n",
        "  title = title.lower()\n",
        "  if len(title) == 0:\n",
        "    return\n",
        "  if title.split()[0].lower()==\"how\" and title.split()[1].lower()==\"to\":\n",
        "    tit += \"You want \"\n",
        "    tit += \" \".join(title.split()[1:])\n",
        "  print(tit)\n",
        "\n",
        "def get_description_title_description(title_description):\n",
        "  if title_description is None or title_description==\"\":\n",
        "    return\n",
        "  print(changePerson(title_description))\n",
        "\n",
        "def get_step_description_game(step, description):\n",
        "  des = description[step]\n",
        "  # print(des)\n",
        "  if(des==\"\"):\n",
        "    return\n",
        "  d = changePerson(des)\n",
        "  return d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y0uDyXmxEwF",
        "colab_type": "text"
      },
      "source": [
        "##Wrong Choices/Similar Steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pebi4f3uwyBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_steps_similar_articles(title, categories=[]):\n",
        "  vectors = Magnitude(\"drive/Shared drives/CIS 700 Project/uniemb.magnitude\")\n",
        "  # v=construct_sentence_vector(title, vectors)\n",
        "  sen_vec = embed([title])\n",
        "  v = np.array(sen_vec).tolist()[0]\n",
        "  sim = vectors.most_similar(v, topn = 10)\n",
        "  steps = []\n",
        "  cnt=0\n",
        "  \n",
        "  for s in sim:\n",
        "    f = open('drive/Shared drives/CIS 700 Project/WikihowData/js_files/' + s[0] + '.json')\n",
        "    data_json = json.load(f) \n",
        "    \n",
        "    tit = data_json['title']\n",
        "    cats = data_json['category_hierarchy']\n",
        "    if categories==None or len(categories)==0 or len(list(set(cats) & set(categories)))>0:\n",
        "      if tit.lower()!=title.lower() and cnt<3:\n",
        "        cnt+=1\n",
        "        if len(data_json['method/part']) > 0:\n",
        "          methods = data_json['method/part']\n",
        "          for method in methods:\n",
        "              all_steps = [step['headline'] for step in method['steps']]\n",
        "              steps += all_steps \n",
        "        else:\n",
        "          if data_json['steps']:\n",
        "            for step in data_json['steps']:\n",
        "              steps.append(step['headline'])       \n",
        "  return steps\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_EqOkHPxWAU",
        "colab_type": "text"
      },
      "source": [
        "## Subcategories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7Fd01u2xJYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stored the sub categories data in categories.json\n",
        "#just load the data for categories \n",
        "with open('drive/Shared drives/CIS 700 Project/categories.json', 'r') as f:\n",
        "  sub_categories = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4dmoU9dxdsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in sub_categories.keys():\n",
        "  print(k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B22hWf3GyDm9",
        "colab_type": "code",
        "outputId": "4f0e7b7f-e51e-47a6-a494-71bb6c49ab9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "total_files = 0\n",
        "for k in sub_categories.keys():\n",
        "  total_files += len(sub_categories[k])\n",
        "print(total_files)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMSzvPKfzGtM",
        "colab_type": "text"
      },
      "source": [
        "## Gameplay functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWNVIqE3yeRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trim_beginning_punctuation(sentence):\n",
        "  to_return = ''\n",
        "  letter_seen = False\n",
        "  for c in sentence:\n",
        "    if c in string.punctuation or c in string.whitespace:\n",
        "      if letter_seen:\n",
        "        to_return += c\n",
        "    else:\n",
        "      if c in string.ascii_letters or c in string.digits:\n",
        "        if not letter_seen:\n",
        "          letter_seen = True\n",
        "        to_return += c\n",
        "  return to_return\n",
        "\n",
        "def milestone_1_results(filename, include_desc):\n",
        "  # include_desc -> Do you want to include imperative description sentences as game steps?\n",
        "  steps, descriptions, method_names, title, category_list, title_dis = get_article(filename)\n",
        "  # Once the necessary fields are filled, find imperatives\n",
        "  imperatives = []\n",
        "\n",
        "  for step in steps:\n",
        "    # First split the headline into sentences\n",
        "    headline_sentences = step[1].split('.')\n",
        "    if len(headline_sentences) == 1:\n",
        "      headline_sentences = headline_sentences[0].split(':')\n",
        "    for headline in headline_sentences:\n",
        "      trimmed_headline = trim_beginning_punctuation(headline)\n",
        "      trimmed_headline = trimmed_headline.replace('\\n', '')\n",
        "      if len(trimmed_headline) == 0:\n",
        "          continue\n",
        "      if imperative_check(trimmed_headline):\n",
        "        if trimmed_headline not in imperatives:\n",
        "          imperatives.append(trimmed_headline)\n",
        "          # fix the way it is represented in descriptions\n",
        "          old_desc = descriptions[step[1]]\n",
        "          del descriptions[step[1]]\n",
        "          descriptions[trimmed_headline] = old_desc\n",
        "        break # No two imperatives from the same headline are added.\n",
        "    if include_desc:\n",
        "      description = descriptions[step[1]]\n",
        "      # Split the description paragraph into sentences\n",
        "      sentences = description.split('.')\n",
        "      for sentence in sentences:\n",
        "        trimmed_sentence = trim_beginning_punctuation(sentence)\n",
        "        trimmed_sentence = trimmed_sentence.replace('\\n', '')\n",
        "        if len(trimmed_sentence) == 0:\n",
        "          continue\n",
        "        if imperative_check(trimmed_sentence):\n",
        "          if trimmed_sentence not in imperatives:\n",
        "            imperatives.append(trimmed_sentence)\n",
        "          break # No two imperatives from the same description are added. !!! REMOVE THIS IF you want multiple imperatives\n",
        "    \n",
        "  #third_person = [changePerson(imperative) for imperative in imperatives]\n",
        "  return imperatives, title, descriptions, category_list, title_dis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0sbB8n4zW8Z",
        "colab_type": "text"
      },
      "source": [
        "Choice picking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61YW2NDTzQrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pick_choices(current_step, commands):\n",
        "  if len(commands) - 1 == current_step:\n",
        "    return None, None\n",
        "  elif len(commands) - 2 == current_step:\n",
        "    # Can only pick one wrong choice\n",
        "    return len(commands) - 1, None\n",
        "  else:\n",
        "    # pick 2 that are not in steps_seen\n",
        "    options_first = [num for num in range(current_step + 1, len(commands))]\n",
        "    first = random.choice(options_first)\n",
        "    options_second = [num for num in range(current_step + 1, len(commands)) if num != first]\n",
        "    second = random.choice(options_second)\n",
        "    return first,second\n",
        "\n",
        "def pick_choices_alt(title, n, similars, curr_step, further_steps):\n",
        "  # first filter the ones that are too similar to actual correct step\n",
        "  user_sent = embed([curr_step])\n",
        "  user_sent = np.array(user_sent).tolist()[0]\n",
        "  # user_sent = construct_sentence_vector(curr_step)\n",
        "  similar_steps = embed(similars)\n",
        "  similar_steps = np.array(similar_steps).tolist()\n",
        "\n",
        "  s = []\n",
        "  for idx, k in enumerate(similar_steps):\n",
        "    a = cosine(user_sent, k)\n",
        "    if a<0.9 and a>0.1:\n",
        "      s.append(similars[idx])\n",
        "  # s = [sim for sim in similars if vectors.similarity(construct_sentence_vector(sim), user_sent) < 0.9 and vectors.similarity(construct_sentence_vector(sim), user_sent) > 0.1]\n",
        "  s = s + further_steps\n",
        "  first = random.choice(s)\n",
        "  s_2 = [step for step in s if step != first]\n",
        "  second = random.choice(s_2)\n",
        "  return first,second"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_F-8Mua1DtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrong_warning = ['Whoa! You should not do that yet.', 'Umm.. maybe you should not do that now.', 'That could work in an ideal world, but that is not what we have here. Try again please!', 'No no no...']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8-bG0X62m0k",
        "colab_type": "text"
      },
      "source": [
        "##Game Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbm51bPG2ktz",
        "colab_type": "code",
        "outputId": "cb1f7e6d-9e92-4632-bdf7-5fe07f88439d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "current_step = 0\n",
        "steps_seen = []\n",
        "end_game = False\n",
        "file_name = ''\n",
        "# First get the game\n",
        "print('Welcome to WikiHow Dungeon!')\n",
        "print('You can either pick a category to play a game from or you can upload your own game.')\n",
        "print('a) Pick a category')\n",
        "print('b) Upload JSON')\n",
        "picked = ''\n",
        "is_picked = False\n",
        "while not is_picked:\n",
        "  picked = input('>')\n",
        "  if picked.lower() == 'a' or picked.lower() == 'b':\n",
        "    is_picked = True\n",
        "  else:\n",
        "    print('Try again.')\n",
        "# If from category\n",
        "if picked.lower() == 'a':\n",
        "  print('Enter the corresponding letter for a category below:')\n",
        "  print('a) School: university, other school stuff')\n",
        "  print('b) Youth: dating, culture, interactions, social events')\n",
        "  print('c) Work: going to work, socializing at work')\n",
        "  print('d) Animals: dogs, cats, birds')\n",
        "  print('e) Fun: activities, parties, toys, music, drinks, games, halloween')\n",
        "  print('f) Home and Family: parents, weddings, neighbors')\n",
        "  print('g) Pop Culture: fandom, celebrities')\n",
        "  print('h) Relationships: single life, dating, relationship issues')\n",
        "  print('i) Friendship: friends, traveling with companions, nicknames')\n",
        "  print('k) Appearance: fashion, looking good')\n",
        "  is_picked = False\n",
        "  while not is_picked:\n",
        "    picked = input('>')\n",
        "    if len(picked) == 1 and picked.lower() in 'abcdefghik':\n",
        "      is_picked = True\n",
        "    else:\n",
        "      print('Try again.')\n",
        "  categories = []\n",
        "  if picked.lower() == 'a':\n",
        "    categories = ['University', 'School Stuff']\n",
        "  elif picked.lower() == 'b':\n",
        "    categories = ['Youth Dating', 'Youth Culture', 'Social Interactions for Youth', 'Social Events for Youth']\n",
        "  elif picked.lower() == 'c':\n",
        "    categories = ['Job Attendance', 'Socializing at Work', 'School Stuff']\n",
        "  elif picked.lower() == 'd':\n",
        "    categories = ['Dogs', 'Cats', 'Birds']\n",
        "  elif picked.lower() == 'e':\n",
        "    categories = ['Drinks', 'Games', 'Parties', 'Fun Activities', 'Toys', 'Halloween', 'Music']\n",
        "  elif picked.lower() == 'f':\n",
        "    categories = ['Youth and Family', 'Parents', 'School Stuff', 'Weddings', 'Neighbors']\n",
        "  elif picked.lower() == 'g':\n",
        "    categories = ['Celebrities', 'Fandom']\n",
        "  elif picked.lower() == 'h':\n",
        "    categories = ['Relationship Issues', 'Single Life', 'Dating']\n",
        "  elif picked.lower() == 'i':\n",
        "    categories = ['Friends', 'Social Interactions', 'Traveling with Companions', 'Nicknames']\n",
        "  elif picked.lower() == 'k':\n",
        "    categories = ['Fashion', 'Looking Good']\n",
        "  c = random.choice(categories)\n",
        "  articles = sub_categories[c]\n",
        "  found_imperatives = False\n",
        "  while not found_imperatives:\n",
        "    file_name = random.choice(articles)\n",
        "    file_name = \"/content/drive/Shared drives/CIS 700 Project/category_files/\" + file_name\n",
        "    # Get the commands, descriptions and title\n",
        "    known_commands, title, descriptions, category_list, title_description = milestone_1_results(file_name, False)\n",
        "    if len(known_commands) > 0:\n",
        "      found_imperatives = True\n",
        "\n",
        "elif picked.lower() == 'b':\n",
        "  # Upload a file\n",
        "  uploaded = files.upload()\n",
        "  try:\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "  except:\n",
        "    print('File not uploaded, will play the default file.')\n",
        "    f = open('/content/3029.json')\n",
        "    file_name = f.name.split('/')\n",
        "    file_name = file_name[len(file_name) - 1]\n",
        "  # Get the commands, descriptions and title\n",
        "  known_commands, title, descriptions, category_list, title_description = milestone_1_results(file_name, False)\n",
        "\n",
        "print('Game: ' + title + '\\n')\n",
        "get_description_title(title)\n",
        "get_description_title_description(title_description)\n",
        "similars = find_steps_similar_articles(title)\n",
        "#similars = []\n",
        "while not end_game:\n",
        "  if current_step == len(known_commands):\n",
        "    end_game = True\n",
        "    print('You reached the end of the game')\n",
        "  else:\n",
        "    if current_step == 0:\n",
        "      print('What do you do first?')\n",
        "    else:\n",
        "      print('What do you do next?')\n",
        "    print('You:')\n",
        "    #first, second = pick_choices(current_step, known_commands)\n",
        "    choices = []\n",
        "    correct_choice = known_commands[current_step] # this is the correct choice\n",
        "    further_steps = [known_commands[i] for i in range(current_step + 1, len(known_commands))]\n",
        "    f, s = pick_choices_alt(title, 2, similars, correct_choice, further_steps)\n",
        "    #f, s = pick_choices(curr_step, known_commands)\n",
        "    if f:\n",
        "      choices.append(f)\n",
        "    if s:\n",
        "      choices.append(s)\n",
        "    choices.append(correct_choice)\n",
        "    random.shuffle(choices) # shuffle the order of choices\n",
        "    for choice in choices:\n",
        "      print('\\t' + choice.lower())\n",
        "    correct_found = False\n",
        "    while not correct_found:\n",
        "      user_pick = input('>')\n",
        "      if user_pick.lower() == 'quit':\n",
        "        correct_found = True\n",
        "        end_game = True\n",
        "        print('You ended the game.')\n",
        "        continue\n",
        "      # Check if what the user picked is correct\n",
        "      most_similar = find_similar_command(user_pick, [known_commands[i] for i in range(current_step, len(known_commands))])\n",
        "      if most_similar.lower() == correct_choice.lower():\n",
        "        # Correct!\n",
        "        current_step += 1\n",
        "        correct_found = True\n",
        "        # Get description\n",
        "        print(get_step_description_game(correct_choice, descriptions))\n",
        "      else:\n",
        "        print(random.choice(wrong_warning))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome to WikiHow Dungeon!\n",
            "You can either pick a category to play a game from or you can upload your own game.\n",
            "a) Pick a category\n",
            "b) Upload JSON\n",
            ">a\n",
            "Enter the corresponding letter for a category below:\n",
            "a) School: university, other school stuff\n",
            "b) Youth: dating, culture, interactions, social events\n",
            "c) Work: going to work, socializing at work\n",
            "d) Animals: dogs, cats, birds\n",
            "e) Fun: activities, parties, toys, music, drinks, games, halloween\n",
            "f) Home and Family: parents, weddings, neighbors\n",
            "g) Pop Culture: fandom, celebrities\n",
            "h) Relationships: single life, dating, relationship issues\n",
            "i) Friendship: friends, traveling with companions, nicknames\n",
            "k) Appearance: fashion, looking good\n",
            ">i\n",
            "Game: How to Interact With Geeks (and Appear Intelligent to Them)\n",
            "\n",
            "You want to interact with geeks (and appear intelligent to them)\n",
            "How does a person interact smoothly with and befriend geeks? This article is written by a geek.\n",
            "The material mostly assumes a school setting, since that is where geeks are most obvious.\n",
            "(Please realize geeks are not considered \"losers\" or \"socially unacceptable\" because that is bullying, and we need respect).\n",
            "\n",
            "What do you do first?\n",
            "You:\n",
            "\tpossess a little random knowledge\n",
            "\tacknowledge your strengths\n",
            "\tenunciate and speak clearly\n",
            ">possess a little random knowledge\n",
            "Stuff like \"It is socially unacceptable to blow your nose in Japan but OK to pick it\".\n",
            "that fascinates geeks, who trade the facts around like bubble gum.\n",
            "\n",
            "What do you do next?\n",
            "You:\n",
            "\tenunciate and speak clearly\n",
            "\tdon't cuss\n",
            "\tdon’t pretend\n",
            ">enunciate and speak clearly\n",
            "You please.\n",
            "This should go without saying but doesn't.\n",
            "It makes you sound like a dweeb by a geek's OR nerd's definition, and they'll avoid you.\n",
            "This says something, because geeks are usually pretty easygoing unless you're mean to them.\n",
            "\n",
            "What do you do next?\n",
            "You:\n",
            "\tbelieve you are smart\n",
            "\tdon't assume that geeks are not sociable\n",
            "\tspeak only when you have something substantial to contribute\n",
            ">speak only when you have something substantial to contribute\n",
            "No no no...\n",
            ">don't assume that geeks are not sociable\n",
            "Just because a geek spends 4+ hours on a computer daily, can Photoshop stuff, uses good grammar and has a 70 wpm typing speed, that they can't socially interact.\n",
            "The reason you don't see geeks chatting away with five people at once all the time is that they don't want or need to--not that they can't or are too shy to.\n",
            "\n",
            "What do you do next?\n",
            "You:\n",
            "\tread, read, read\n",
            "\tdon’t compare yourself to others\n",
            "\tdon't assume that glasses+braces=geek or nerd\n",
            ">don't assume that glasses+braces=geek or nerd\n",
            "Geeks and nerds are very proud of their intellect and unusual hobbies.\n",
            "There are a lot of idiots in this world who wear glasses or braces, and geeks and nerds hate to be associated with them.\n",
            "Possession of glasses or braces doesn't relate to geekiness at all, and having them doesn't magically cause a person to become a geek.\n",
            "Sometimes people even make efforts not to become a geek when they get glasses because they are afraid of being an outcast.\n",
            "\n",
            "What do you do next?\n",
            "You:\n",
            "\tdo not interrupt a geek that is busy\n",
            "\tdo extra research\n",
            "\ttreat smart people as equals\n",
            ">quit\n",
            "You ended the game.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}